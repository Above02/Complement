{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprender e implementar redes neuronales desde cero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contenido\n",
    "\n",
    "\n",
    "1. ¿Qué son las Redes Neuronales?\n",
    "\n",
    "2. Implementar una Red Neuronal - Clasificación binaria\n",
    "\n",
    "3. Implementar una Red Neuronal - Clasificación Multiclase\n",
    "\n",
    "4. ¿Qué son las redes neuronales profundas?\n",
    "\n",
    "5. Implementación de Redes Neuronales Convolucionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "1. ¿Qué son las Redes Neuronales?\n",
    "\n",
    "Las redes neuronales son un tipo de modelos de aprendizaje automático que están diseñados para **operar de manera similar a las neuronas biológicas y al sistema nervioso humano**. \n",
    "\n",
    "Estos modelos se utilizan para reconocer patrones y relaciones complejas que existen dentro de un conjunto de datos etiquetado. Tienen las siguientes propiedades:\n",
    "\n",
    "    La arquitectura central de un modelo de red neuronal se compone de una gran cantidad de nodos de procesamiento simples llamados neuronas que están interconectados y organizados en diferentes capas.\n",
    "\n",
    "    Un nodo individual en una capa está conectado a varios otros nodos en la capa anterior y siguiente. Las entradas de una capa se reciben y procesan para generar la salida que se pasa a la siguiente capa.\n",
    "\n",
    "    La primera capa de esta arquitectura a menudo se denomina capa de entrada que acepta las entradas, la última capa se denomina capa de salida que produce la salida y todas las demás capas entre la capa de entrada y la capa de salida se denominan capas ocultas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conceptos clave en una Red Neuronal\n",
    "\n",
    "**A. Neurona:**\n",
    "\n",
    "Una neurona es una unidad de procesamiento única de una red neuronal que está conectada a otras neuronas diferentes en la red. Estas conexiones representan entradas y salidas de una neurona. A cada una de sus conexiones, la neurona asigna un *peso* (W) que significa la importancia de la entrada y agrega un término de sesgo (b).\n",
    "\n",
    "\n",
    "**B. Funciones de activación**\n",
    "\n",
    "Las funciones de activación se utilizan para aplicar una transformación no lineal en la entrada para asignarla a la salida. El objetivo de las funciones de activación es predecir la clase correcta de la variable de destino en función de la combinación de entrada de variables. Algunas de las funciones de activación populares son Relu, Sigmoid y TanH.\n",
    "\n",
    "**C. Propagación hacia adelante**\n",
    "\n",
    "El modelo de red neuronal pasa por el proceso llamado propagación directa en el que pasa las salidas de activación calculadas en la dirección adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Z = W*X + b$\n",
    "\n",
    "\n",
    "$A = g(Z)$\n",
    "\n",
    "    g es la función de activación\n",
    "    A es la activación usando la entrada\n",
    "    W es el peso asociado con la entrada\n",
    "    B es el sesgo asociado con el nodo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D. Cálculo de errores:**\n",
    "\n",
    "La red neuronal aprende mejorando los valores de pesos y sesgos. El modelo calcula el error en la salida predicha en la capa final que luego se usa para hacer pequeños ajustes en los pesos y el sesgo. Los ajustes se realizan de manera que se minimice el error total. La **función de pérdida** mide el error en la capa final y la **función de costo** mide el error total de la red.\n",
    "\n",
    "$Pérdida = Valor Real - Valor Predicho$\n",
    "\n",
    "$Costo = Suma (Pérdida)$\n",
    "\n",
    "**E. Propagación hacia atrás:**\n",
    "\n",
    "El modelo de red neuronal se somete al proceso llamado **retropropagación** en el que el error se pasa a las capas hacia atrás para que esas capas también puedan mejorar los valores asociados de pesos y sesgos. Utiliza el algoritmo llamado **Gradient Descent** en el que se minimiza el error y se obtienen valores óptimos de pesos y sesgos. Este ajuste de pesos y sesgos se realiza calculando la derivada del error, la derivada de los pesos, el sesgo y restándolos de los valores originales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "2. Implementar una Red Neuronal - Clasificación Binaria\n",
    "\n",
    "Implementemos una red neuronal básica en python para la clasificación binaria que se usa para clasificar si una imagen dada es 0 o 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow==2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Preparación del conjunto de datos\n",
    "\n",
    "El primer paso es cargar y preparar el conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../datasets/train.csv\")\n",
    "test = pd.read_csv(\"../datasets/test.csv\")\n",
    "\n",
    "# include only the rows having label = 0 or 1 (binary classification)\n",
    "X = train[train['label'].isin([0, 1])]\n",
    "\n",
    "# target variable\n",
    "Y = train[train['label'].isin([0, 1])]['label']\n",
    "\n",
    "# remove the label from X\n",
    "X = X.drop(['label'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Implementando una Función de Activación\n",
    "\n",
    "Usaremos la función de activación sigmoide porque genera valores entre 0 y 1, por lo que es una buena opción para un problema de clasificación binaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing a sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    s = 1.0/ (1 + np.exp(-z))    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Definir la arquitectura de la red neuronal\n",
    "\n",
    "Crea un modelo con tres capas: Entrada, Oculta, Salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_architecture(X, Y):\n",
    "    # nodes in input layer\n",
    "    n_x = X.shape[0] \n",
    "    # nodes in hidden layer\n",
    "    n_h = 10          \n",
    "    # nodes in output layer\n",
    "    n_y = Y.shape[0] \n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4 Definir los parámetros de la red neuronal\n",
    "\n",
    "Los parámetros de la red neuronal son pesos y sesgos que necesitamos inicializar con valores cero. La primera capa solo contiene entradas, por lo que no hay pesos ni sesgos, pero la capa oculta y la capa de salida tienen un término de peso y sesgo. (W1, b1 y W2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_network_parameters(n_x, n_h, n_y):\n",
    "    W1 = np.random.randn(n_h,n_x) * 0.01 # random initialization\n",
    "    b1 = np.zeros((n_h, 1)) # zero initialization\n",
    "    W2 = np.random.randn(n_y,n_h) * 0.01 \n",
    "    b2 = np.zeros((n_y, 1)) \n",
    "    return {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5 Implementar la propagación hacia adelante\n",
    "\n",
    "La capa oculta y la capa de salida calcularán las activaciones utilizando la función de activación sigmoide y la pasarán en la dirección de avance. Mientras se calcula esta activación, la entrada se multiplica por peso y se suma con sesgo antes de pasarla a la función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, params):\n",
    "    Z1 = np.dot(params['W1'], X)+params['b1']\n",
    "    A1 = sigmoid(Z1)\n",
    "\n",
    "    Z2 = np.dot(params['W2'], A1)+params['b2']\n",
    "    A2 = sigmoid(Z2)\n",
    "    return {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.6 Calcular el error de red\n",
    "\n",
    "Para calcular el costo, un enfoque sencillo es calcular el error absoluto entre la predicción y el valor real. Pero una mejor función de pérdida es la función log que se define como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(Predicted, Actual):\n",
    "    logprobs = np.multiply(np.log(Predicted), Actual)+ np.multiply(np.log(1-Predicted), 1-Actual)\n",
    "    cost = -np.sum(logprobs) / Actual.shape[1] \n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.7 Implementar la propagación hacia atrás\n",
    "\n",
    "En la función de propagación hacia atrás, el error se pasa hacia atrás a las capas anteriores y se calculan las derivadas de los pesos y el sesgo. Luego, los pesos y el sesgo se actualizan utilizando las derivadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(params, activations, X, Y):\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # output layer\n",
    "    dZ2 = activations['A2'] - Y # compute the error derivative \n",
    "    dW2 = np.dot(dZ2, activations['A1'].T) / m # compute the weight derivative \n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True)/m # compute the bias derivative\n",
    "    \n",
    "    # hidden layer\n",
    "    dZ1 = np.dot(params['W2'].T, dZ2)*(1-np.power(activations['A1'], 2))\n",
    "    dW1 = np.dot(dZ1, X.T)/m\n",
    "    db1 = np.sum(dZ1, axis=1,keepdims=True)/m\n",
    "    \n",
    "    return {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "\n",
    "def update_parameters(params, derivatives, alpha = 1.2):\n",
    "    # alpha is the model's learning rate \n",
    "    \n",
    "    params['W1'] = params['W1'] - alpha * derivatives['dW1']\n",
    "    params['b1'] = params['b1'] - alpha * derivatives['db1']\n",
    "    params['W2'] = params['W2'] - alpha * derivatives['dW2']\n",
    "    params['b2'] = params['b2'] - alpha * derivatives['db2']\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.8 Compilar y entrenar el modelo\n",
    "\n",
    "Crea una función que compile todas las funciones clave y crea un modelo de red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(X, Y, n_h, num_iterations=100):\n",
    "    n_x = network_architecture(X, Y)[0]\n",
    "    n_y = network_architecture(X, Y)[2]\n",
    "    \n",
    "    params = define_network_parameters(n_x, n_h, n_y)\n",
    "    for i in range(0, num_iterations):\n",
    "        results = forward_propagation(X, params)\n",
    "        error = compute_error(results['A2'], Y)\n",
    "        derivatives = backward_propagation(params, results, X, Y) \n",
    "        params = update_parameters(params, derivatives)    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "y = Y.values.reshape(1, Y.size)\n",
    "x = X.T\n",
    "model = neural_network(x, y, n_h = 10, num_iterations = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.9 Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.80148908 0.06887999 0.90268893 ... 0.80148908 0.06887999 0.90268893]\n",
      "Accuracy: 95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "def predict(parameters, X):\n",
    "    results = forward_propagation(X, parameters)\n",
    "    print (results['A2'][0])\n",
    "    predictions = np.around(results['A2'])    \n",
    "    return predictions\n",
    "\n",
    "predictions = predict(model, x)\n",
    "print ('Accuracy: %d' % float((np.dot(y,predictions.T) + np.dot(1-y,1-predictions.T))/float(y.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "3. Implementar una Red Neuronal - Clasificación Multiclase\n",
    "\n",
    "En el paso anterior, implementamos una NN para la clasificación binaria en python desde cero. Las bibliotecas de Python, como sklearn, proporcionan una excelente implementación de redes neuronales eficientes que se pueden usar para implementar directamente redes neuronales en un conjunto de datos. En esta sección, implementemos una red neuronal multiclase para clasificar el dígito que se muestra en una imagen del 0 al 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Preparación del conjunto de datos\n",
    "\n",
    "Cortar el conjunto de datos del entrenamiento en un conjunto de entrenamiento y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import neural_network\n",
    "from sklearn import  metrics\n",
    "\n",
    "Y = train['label'][:10000] # use more number of rows for more training \n",
    "X = train.drop(['label'], axis = 1)[:10000] # use more number of rows for more training \n",
    "x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Entrenar el modelo\n",
    "\n",
    "Entrene un modelo de red neuronal con 10 capas ocultas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=1e-05, hidden_layer_sizes=(5,), random_state=18,\n",
       "              solver='lbfgs')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(alpha=1e-5, hidden_layer_sizes=(5,), solver='lbfgs', random_state=18)\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       186\n",
      "           1       0.96      0.86      0.91       210\n",
      "           2       0.12      0.99      0.21       220\n",
      "           3       0.00      0.00      0.00       190\n",
      "           4       0.00      0.00      0.00       188\n",
      "           5       0.00      0.00      0.00       194\n",
      "           6       0.00      0.00      0.00       190\n",
      "           7       0.00      0.00      0.00       233\n",
      "           8       0.00      0.00      0.00       197\n",
      "           9       0.00      0.00      0.00       192\n",
      "\n",
      "    accuracy                           0.20      2000\n",
      "   macro avg       0.11      0.18      0.11      2000\n",
      "weighted avg       0.11      0.20      0.12      2000\n",
      ":\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict(x_val)\n",
    "print(\"Classification Report:\\n %s:\" % (metrics.classification_report(y_val, predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Redes neuronales profundas - Redes neuronales convolucionales\n",
    "\n",
    "Las Redes Neuronales Profundas están compuestas por muchas y complejas capas ocultas que intentan extraer características de bajo nivel de las imágenes. Algunos ejemplos de redes neuronales profundas complejas son las redes neuronales convolucionales y las redes neuronales recurrentes.\n",
    "\n",
    "\n",
    "**Redes neuronales convolucionales**\n",
    "\n",
    "En las redes neuronales convolucionales, cada entrada de imagen se trata como una matriz de valores de píxeles que representa la cantidad de oscuridad en un píxel determinado de la imagen. A diferencia de las redes neuronales tradicionales que tratan una imagen como una red unidimensional, las CNN consideran la ubicación de los píxeles y los vecinos para la clasificación.\n",
    "\n",
    "![img](https://www.mdpi.com/information/information-07-00061/article_deploy/html/images/information-07-00061-g001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Componentes clave de la red neuronal convolucional.\n",
    "\n",
    "**A. Capa convolucional:** en esta capa, se utiliza una matriz de kernel (o peso) para extraer características de bajo nivel de las imágenes. El núcleo con sus pesos gira sobre la matriz de la imagen en forma de ventana deslizante para obtener la salida convolucionada. La matriz del kernel se comporta como un filtro en una imagen extrayendo información particular de la matriz de la imagen original. Durante el proceso de colvolución, los pesos se aprenden de tal manera que se minimiza la función de pérdida.\n",
    "\n",
    "**B. Stride:** Stride se define como el número de pasos que toma el kernel o la matriz de peso mientras se mueve por toda la imagen moviendo N píxeles a la vez. Si la matriz de peso mueve N píxeles a la vez, se llama Stride de N.\n",
    "\n",
    "Créditos de imagen - www.deeplearning.net\n",
    "\n",
    "**C. Capa de agrupación (Pooling):** las capas de agrupación (Pooling) se utilizan para extraer las características más informativas de la salida convolucionada generada.\n",
    "\n",
    "![img](https://upload.wikimedia.org/wikipedia/commons/e/e9/Max_pooling.png)\n",
    "\n",
    "**D. Capa de salida:** para generar la salida final, se aplica una capa densa o completamente conectada con la función de activación softmax. La función Softmax se usa para generar las probabilidades para cada clase de la variable objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Implementar una red neuronal de convolución\n",
    "\n",
    "5.1 Preparación del conjunto de datos\n",
    "\n",
    "En el primer paso, preparemos el conjunto de datos y dividámoslo en conjuntos de entrenamiento y validación. Para fines de modelado y entrenamiento, podemos usar la biblioteca de python: Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = train['label']\n",
    "X = train.drop(['label'], axis=1)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(X.values, Y, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2 Definir los parámetros de red\n",
    "\n",
    "Los parámetros de red son:\n",
    "\n",
    "**Tamaño del lote (Batch size):** define el número de muestras que se propagarán a través de la red. Mientras más grande el tamaño del lote, más memoria se ocupará.\n",
    "\n",
    "**Número de clases(Num Classes):**  número total de clases posibles en la variable de destino\n",
    "\n",
    "**Épocas (Epochs):** número total de iteraciones para las que se ejecutará el modelo cnn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, supongamos que tenemos 1050 muestras de entrenamiento y deseamos configurar un tamaño de lote igual a 100. \n",
    "\n",
    "El algoritmo toma las primeras 100 muestras (del 1 al 100) del conjunto de datos de entrenamiento y entrena la red. Luego, toma las segundas 100 muestras (del 101 al 200) y vuelve a entrenar la red. Podemos seguir haciendo este procedimiento hasta que hayamos propagado todas las muestras a través de la red. El problema podría ocurrir con el último conjunto de muestras. En nuestro ejemplo, hemos usado 1050 que no es divisible por 100 sin resto. La solución más simple es simplemente obtener las 50 muestras finales y entrenar la red.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ventajas de usar un tamaño de lote < número de todas las muestras:**\n",
    "\n",
    "    Requiere menos memoria. Dado que entrena la red con menos muestras, el procedimiento de entrenamiento general requiere menos memoria. Eso es especialmente importante si no puede colocar todo el conjunto de datos en la memoria de su máquina.\n",
    "\n",
    "    Por lo general, las redes se entrenan más rápido con mini lotes. Eso es porque actualizamos los pesos después de cada propagación. En nuestro ejemplo, propagamos 11 lotes (10 de ellos tenían 100 muestras y 1 tenía 50 muestras) y después de cada uno de ellos actualizamos los parámetros de nuestra red. Si usáramos todas las muestras durante la propagación, haríamos solo 1 actualización para el parámetro de la red.\n",
    "\n",
    "Desventajas de usar un tamaño de lote < número de todas las muestras:\n",
    "\n",
    "    Cuanto más pequeño sea el lote, menos precisa será la estimación del gradiente. En la figura a continuación, puede ver que la dirección del gradiente del mini lote (color verde) fluctúa mucho más en comparación con la dirección del gradiente del lote completo (color azul).\n",
    "    \n",
    "\n",
    "![img](https://i.stack.imgur.com/lU3sx.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network parameters \n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 5 # Further Fine Tuning can be done\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3 Preprocesar las entradas\n",
    "\n",
    "En el paso de preprocesamiento, los vectores de datos de imagen correspondientes se reforman en un vector de 4 dimensiones: tamaño total del lote, ancho de la imagen, altura de la imagen y canal. En nuestro caso, channel = 1 ya que solo usaremos un solo canal en lugar de tres canales (R, G, B). El siguiente paso es normalizar las entradas dividiéndolas por el valor máximo de píxeles, es decir 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the train data \n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255\n",
    "\n",
    "# preprocess the validation data\n",
    "x_val = x_val.reshape(x_val.shape[0], img_rows, img_cols, 1)\n",
    "x_val = x_val.astype('float32')\n",
    "x_val /= 255\n",
    "\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# convert the target variable \n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "\n",
    "# preprocess the test data\n",
    "Xtest = test.values\n",
    "Xtest = Xtest.reshape(Xtest.shape[0], img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.4 Crear la arquitectura del modelo CNN\n",
    "\n",
    "En este paso, creamos la arquitectura de red neuronal convolucional con las siguientes capas:\n",
    "\n",
    "    Capa convolucional con tamaño de núcleo = 3*3, 32 unidades convolucionales y función de activación RelU\n",
    "    Capa convolucional con tamaño de kernel = 3*3, 64 unidades convolucionales y función de activación RelU\n",
    "    Capa de agrupación (Poolong) máxima con tamaño de matriz de agrupación = 2*2\n",
    "    Capa de abandono (Dropout): se utiliza una capa de abandono para regularizar y reducir el sobreajuste\n",
    "    Capa aplanada (Flatten): una capa para convertir la salida en una matriz unidimensional\n",
    "    Capa densa: una capa densa es una capa completamente conectada en la que cada nodo está conectado a todos los demás nodos en las capas anterior y siguiente. En nuestra red, contiene 128 neuronas, pero este número se puede cambiar para otros experimentos.\n",
    "    Otra capa de abandono (dropout) para la regularización\n",
    "    Capa de salida final: una capa densa con 10 neuronas para generar la clase de salida\n",
    "\n",
    "En la red neuronal simple que implementamos en el paso 1, la función de pérdida era la función Log y el algoritmo de optimización era Gradient Descent. En esta red neuronal, usaremos **categorical_crossentropy**, ya que esta es una clasificación de varias clases, como función de pérdida y **Adadelta** como función de optimización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# add first convolutional layer\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "\n",
    "# add second convolutional layer\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# add one max pooling layer \n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# add one dropout layer\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# add flatten layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# add dense layer\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# add another dropout layer\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# add dense layer\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# complile the model and view its architecur\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,  optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.5 Entrenamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "296/296 [==============================] - 27s 91ms/step - loss: 2.2962 - accuracy: 0.1233 - val_loss: 2.2679 - val_accuracy: 0.2529\n",
      "Epoch 2/5\n",
      "296/296 [==============================] - 30s 102ms/step - loss: 2.2552 - accuracy: 0.1963 - val_loss: 2.2215 - val_accuracy: 0.4152\n",
      "Epoch 3/5\n",
      "296/296 [==============================] - 30s 101ms/step - loss: 2.2089 - accuracy: 0.2828 - val_loss: 2.1684 - val_accuracy: 0.5086\n",
      "Epoch 4/5\n",
      "296/296 [==============================] - 32s 107ms/step - loss: 2.1549 - accuracy: 0.3533 - val_loss: 2.1038 - val_accuracy: 0.5788\n",
      "Epoch 5/5\n",
      "296/296 [==============================] - 33s 110ms/step - loss: 2.0887 - accuracy: 0.4167 - val_loss: 2.0242 - val_accuracy: 0.6255\n",
      "Test accuracy: 0.6254761815071106\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_val, y_val))\n",
    "accuracy = model.evaluate(x_val, y_val, verbose=0)\n",
    "print('Test accuracy:', accuracy[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.6 Generar Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(Xtest)\n",
    "y_classes = pred.argmax(axis=-1)\n",
    "res = pd.DataFrame()\n",
    "res['ImageId'] = list(range(1,28001))\n",
    "res['Label'] = y_classes\n",
    "res.to_csv(\"output.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27995</th>\n",
       "      <td>27996</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27996</th>\n",
       "      <td>27997</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27997</th>\n",
       "      <td>27998</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27998</th>\n",
       "      <td>27999</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27999</th>\n",
       "      <td>28000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ImageId  Label\n",
       "0            1      2\n",
       "1            2      0\n",
       "2            3      9\n",
       "3            4      9\n",
       "4            5      2\n",
       "...        ...    ...\n",
       "27995    27996      9\n",
       "27996    27997      9\n",
       "27997    27998      3\n",
       "27998    27999      9\n",
       "27999    28000      2\n",
       "\n",
       "[28000 rows x 2 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
